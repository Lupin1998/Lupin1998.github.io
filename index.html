<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="robots" content="index, follow" />
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="keywords" content="Siyuan Li, 李思远, machine learning, westlake university, zhejiang university">
<link rel="stylesheet" href="./Files/jemdoc.css" type="text/css" />
<script src="jquery.min.js"></script>
<link rel="shortcut icon" href="./Files/westlake.ico">
<title>Siyuan Li</title>
<script async defer src="https://buttons.github.io/buttons.js"></script>
</head>


<body>

<a id="home" class="anchor"></a>
<div id="container"> 
<div class="container"> 

<table class="imgtable"><tr><td>
<a href="./" style="color:#2a7ce0"><img src="./Files/lisiyuan.jpg" alt="" height="215px" /></a>&nbsp;</td>
<td align="left"><p><a href="./" style="color:#2a7ce0"><font size="4">Siyuan Li (</font><font size="4"; font style="font-family:Microsoft YaHei">李思远</font><font size="4">)</font></a><br />
<i>Ph.D Candidate, <a href="https://www.zju.edu.cn/" target="_blank">Zhejiang University</a> & <a href="https://www.westlake.edu.cn/" target="_blank">Westlake University</a></i>
<br /><br />
<a href="https://www.westlake.edu.cn/" target="_blank" style="color:#2a7ce0">Center for Artificial Intelligence Research and Innovation (CAIRI AI Lab)</a><br />
<br />
Location: E2-223, Westlake University, Dunyu Road #600, Xihu District, Hangzhou, Zhejiang, China<br />
<!-- Location: Building 2-Room 508, Shilongshan Street #18, Xihu District, Hangzhou, Zhejiang, China<br /> -->
<class="staffshortcut">
 <A HREF="#News" style="color:#2a7ce0">News</A> | 
 <A HREF="#Interest" style="color:#2a7ce0">Research Interest</A> | 
 <A HREF="#Education" style="color:#2a7ce0">Education</A> | 
 <A HREF="#Internship" style="color:#2a7ce0">Internship</A> | 
 <A HREF="#Publications" style="color:#2a7ce0">Publications</A> | 
 <A HREF="#Services" style="color:#2a7ce0">Services</A> | 
 <A HREF="./Files/SiyuanLi_CV.pdf" style="color:#2a7ce0">Resume (CV)</A> | 
 <!-- <A HREF="#Awards" style="color:#2a7ce0">Awards</A> -->
<br />
<br />

Email: lisiyuan@westlake.edu.cn (prior); lsy@smail.nju.edu.cn <br />
[<a href="https://scholar.google.co.id/citations?user=SKTQTXwAAAAJ&hl=zh-CN" target="_blank" style="color:#2a7ce0">Google Scholar</a>] 
[<a href="https://github.com/Lupin1998" target="_blank" style="color:#2a7ce0">GitHub</a>] 
[<a href="https://www.researchgate.net/profile/Siyuan-Li-37" target="_blank" style="color:#2a7ce0">ResearchGate</a>] 
[<a href="https://orcid.org/0000-0001-6806-2468" target="_blank" style="color:#2a7ce0">ORCID</a>] 
[<a href="https://www.semanticscholar.org/author/Siyuan-Li/2118155623" target="_blank" style="color:#2a7ce0">Semantic Scholar</a>] 
[<a href="https://www.linkedin.com/in/siyuan-li-lupin1998/" target="_blank" style="color:#2a7ce0">LinkedIn</a>] 

<br />
<br />
<i>Welcome to contacting me about research cooperation or applying internship. Please drop me emails (lisiyuan@westlake.edu.cn) or WeChat me (Lupin_1998).</i><br />
</td></tr></table>


<A NAME="News"><h2>News</h2></A>
  <!-- <div style="height:200px;overflow-y:auto;background:#ffffff;"> -->
  <div style="height:200px;overflow-y:auto;">
  <ul>
    <li><b> <font color="#b80000">[2025.06]</font> </b> One paper on <i>Multi-task learning</i>, <i><a href="https://arxiv.org/abs/2506.01049">RepMTL</a></i>, is accepted by <b>ICCV 2025</b>, congrats to <a href="https://scholar.google.com/citations?hl=en&user=CEJ4pugAAAAJ" target="_blank" style="color:#2a7ce0">Zedong Wang</a>. </li>
    <li><b> <font color="#b80000">[2025.05]</font> </b> One paper on <i>LLM Optimization</i>, <i><a href="https://arxiv.org/abs/2506.01049"> SGG</a></i>, is accepted by <b>ACL 2025 (main)</b>. </li>
    <li><b> <font color="#b80000">[2025.04]</font> </b> Two co-authored paper on <i>MLLM</i> and <i>AI4Science</i> are accepted by <b>IJCAI 2025</b>, congrats to <a href="https://scholar.google.com/citations?user=6kTV6aMAAAAJ&hl=zh-CN" target="_blank" style="color:#2a7ce0">Cheng Tan</a> and <a href="https://scholar.google.com/citations?hl=zh-CN&user=6FZh9C8AAAAJ" target="_blank" style="color:#2a7ce0">Bozhen Hu</a>. </li>
    <li><b> <font color="#b80000">[2025.03]</font> </b> One paper on <i>Generation & Representation</i>, <i><a href="https://arxiv.org/abs/2504.00999"> MergeVQ</a></i>, is accepted by <b>CVPR 2025</b>. </li>
    <li><b> <font color="#b80000">[2025.03]</font> </b> Two co-authored paper on <i>AIGC</i> are accepted by <b>CVPR 2025</b>, congrats to <a href="https://scholar.google.co.id/citations?user=qmTjdwIAAAAJ&hl=zh-CN" target="_blank" style="color:#2a7ce0">Yufei Huang</a> and <a href="https://scholar.google.com/citations?hl=zh-CN&user=_rbVn8MAAAAJ" target="_blank" style="color:#2a7ce0">Jingxuan Wei</a>. </li>
    <li><b> <font color="#b80000">[2025.01]</font> </b> Three co-authored paper on <i>AI4Science</i> are accepted by <b>ICLR 2025</b>, congrats to <a href="https://scholar.google.co.id/citations?user=3V9HgcwAAAAJ&hl=zh-CN" target="_blank" style="color:#2a7ce0">Di Wu</a>, <a href="https://scholar.google.com/citations?user=6kTV6aMAAAAJ&hl=zh-CN" target="_blank" style="color:#2a7ce0">Cheng Tan</a>, and <a href="https://scholar.google.co.id/citations?user=qmTjdwIAAAAJ&hl=zh-CN" target="_blank" style="color:#2a7ce0">Yufei Huang</a>. </li>
    <li><b> <font color="#b80000">[2024.12]</font> </b> Two co-authored paper on <i>Low-level vision</i> are accepted by <b>AAAI 2025</b>, congrats to <a href="https://openreview.net/profile?id=~Xiongfei_Su1" target="_blank" style="color:#2a7ce0">Xiongfei Su</a>. </li>
    <li><b> <font color="#b80000">[2024.09]</font> </b> Four co-authored paper on <i>AI4Science</i> and <i>Face recognition</i> are accepted by <b>NeurIPS 2024</b>, congrats to <a href="https://scholar.google.com/citations?user=8vHkc5YAAAAJ&hl=en" target="_blank" style="color:#2a7ce0">Fang Wu</a>, <a href="https://jane-pyc.github.io" target="_blank" style="color:#2a7ce0">Chengrui Duan</a>, <a href="https://scholar.google.co.id/citations?user=4SclT-QAAAAJ&hl=zh-CN" target="_blank" style="color:#2a7ce0">Zhangyang Gao</a>, and <a href="https://scholar.google.com.hk/citations?user=NB9Mn5MAAAAJ&hl=zh-CN&oi=ao" target="_blank" style="color:#2a7ce0">Jun Dan</a>. </li>
    <li><b> <font color="#b80000">[2024.08]</font> </b> One paper on <i>GNN theory</i>, <i><a href="https://arxiv.org/abs/2205.07266">GNN Bottleneck</a></i>, is accepted by <b>TKDE 2024</b>, congrats to <a href="https://scholar.google.com/citations?user=8vHkc5YAAAAJ&hl=en" target="_blank" style="color:#2a7ce0">Fang Wu</a>. </li>
    <li><b> <font color="#b80000">[2024.07]</font> </b> Co-authored paper on <i>CoT reasoning</i>, <i><a href="https://arxiv.org/abs/2311.14109">MC-CoT</a></i>, is accepted by <b>ECCV 2024</b>, congrats to <a href="https://scholar.google.com/citations?user=6kTV6aMAAAAJ&hl=zh-CN" target="_blank" style="color:#2a7ce0">Cheng Tan</a>. </li>
    <li><b> <font color="#b80000">[2024.06]</font> </b> Co-authored paper on <i>Neuro-science</i>, <i><a href="https://arxiv.org/abs/2204.12440">NeuroBERT</a></i>, is accepted by <b>JBHI 2024</b>, congrats to <a href="https://scholar.google.co.id/citations?user=3V9HgcwAAAAJ&hl=zh-CN" target="_blank" style="color:#2a7ce0">Di Wu</a>. </li>
    <li><b> <font color="#b80000">[2024.05]</font> </b> One paper on <i>genomics</i>, <i><a href="https://arxiv.org/abs/2405.10812">VQDNA</a></i>, is accepted by <b>ICML 2024</b>. </li>
    <li><b> <font color="#b80000">[2024.05]</font> </b> Three co-authored paper on <i>Long-sequence</i> (<a href="https://arxiv.org/abs/2406.08128">CHELA</a>) and <i>AI4Science</i> are accepted by <b>ICML 2024</b>, congrats to <a href="https://scholar.google.co.id/citations?user=EwMGZsgAAAAJ&hl=zh-CN" target="_blank" style="color:#2a7ce0">Zicheng Liu</a>, <a href="https://scholar.google.com/citations?user=Tk7TrCoAAAAJ&hl=zh-CN" target="_blank" style="color:#2a7ce0">Lirong Wu</a>, and <a href="https://scholar.google.co.id/citations?user=qmTjdwIAAAAJ&hl=zh-CN" target="_blank" style="color:#2a7ce0">Yufei Huang</a>. </li>
    <li><b> <font color="#b80000">[2024.04]</font> </b> Co-authored paper on <i>Long-sequence model</i>, <i><a href="https://arxiv.org/abs/2404.11163"> LongVQ</a></i>, is accepted by <b>IJCAI 2024</b>, congrats to <a href="https://scholar.google.co.id/citations?user=EwMGZsgAAAAJ&hl=zh-CN" target="_blank" style="color:#2a7ce0">Zicheng Liu</a>. </li>
    <li><b> <font color="#b80000">[2024.01]</font> </b> One paper on <i>semi-supervised learning</i>, <i><a href="https://arxiv.org/abs/2310.03013"> SemiReward</a></i>, is accepted by <b>ICLR 2024</b>. </li>
    <li><b> <font color="#b80000">[2024.01]</font> </b> One paper on <i>vision architecture</i>, <i><a href="https://arxiv.org/abs/2211.03295"> MogaNet</a></i>, is accepted by <b>ICLR 2024</b>. </li>
    <li><b> <font color="#b80000">[2024.01]</font> </b> Three co-authored paper on <i>AI4Science</i> are accepted by <b>ICLR 2024</b> (MAPE-PPI wins <font color="#FF0000">Spotlight</font>), congrats to <a href="https://scholar.google.com/citations?user=Tk7TrCoAAAAJ&hl=zh-CN" target="_blank" style="color:#2a7ce0">Lirong Wu</a>, <a href="https://scholar.google.co.id/citations?user=4SclT-QAAAAJ&hl=zh-CN" target="_blank" style="color:#2a7ce0">Zhangyang Gao</a>, and <a href="https://scholar.google.com/citations?user=6kTV6aMAAAAJ&hl=zh-CN" target="_blank" style="color:#2a7ce0">Cheng Tan</a>. </li>
    <li><b> <font color="#b80000">[2023.12]</font> </b> Two co-authored paper on <i>protein</i> and <i>video</i> are accepted by <b>AAAI 2024</b>, congrats to <a href="https://scholar.google.co.id/citations?user=qmTjdwIAAAAJ&hl=zh-CN&oi=sra" target="_blank" style="color:#2a7ce0">Yufei Huang</a> and <a href="https://scholar.google.co.id/citations?hl=zh-CN&user=nBPNLhgAAAAJ" target="_blank" style="color:#2a7ce0">Xuesong Nie</a>. </li>
    <li><b> <font color="#b80000">[2023.10]</font> </b> One paper on <i>self-supervised learning</i>, <i><a href="https://arxiv.org/abs/2110.14553"> GenURL</a></i>, is accepted by <b>TNNLS 2023</b>. </li>
    <li><b> <font color="#b80000">[2023.09]</font> </b> One paper on <i>video prediction</i>, <i><a href="https://arxiv.org/abs/2306.11249"> OpenSTL</a></i>, is accepted by <b>NeurIPS 2023</b>. </li>
    <li><b> <font color="#b80000">[2023.09]</font> </b> One paper on <i>data augmentation</i>, <i><a href="https://arxiv.org/abs/2203.10761"> Decoupled Mixup</a></i>, is accepted by <b>NeurIPS 2023</b>. </li>
    <li><b> <font color="#b80000">[2023.09]</font> </b> Two co-authored paper on <i>AI4Science</i> are accepted by <b>NeurIPS 2023</b>, congrats to <a href="https://scholar.google.co.id/citations?user=o5A23qIAAAAJ&hl=zh-CN" target="_blank" style="color:#2a7ce0">Haitao Lin</a> and <a href="https://scholar.google.com/citations?user=aPKKpSYAAAAJ&hl=en" target="_blank" style="color:#2a7ce0">Jun Xia</a>. </li>
    <li><b> <font color="#b80000">[2023.06]</font> </b> Build an open-source toolbox, <i><a href="https://github.com/chengtan9907/OpenSTL"> OpenSTL</a></i>, for spatio-temporal predictive learning based on PyTorch and update on <i><a href="https://arxiv.org/abs/2306.11249"> arXiv (OpenSTL)</a></i>. On updating! </li>
    <li><b> <font color="#b80000">[2023.06]</font> </b> One paper on <i>label noise</i>, <i><a href="https://ieeexplore.ieee.org/abstract/document/10158394"> GNN-Cleaner</a></i>, is accepted by <b>TKDE 2023</b>, congrats to <a href="https://scholar.google.com/citations?user=aPKKpSYAAAAJ&hl=en" target="_blank" style="color:#2a7ce0">Jun Xia</a>. </li>
    <li><b> <font color="#b80000">[2023.04]</font> </b> One paper on <i>self-supervised learning</i>, <i><a href="https://arxiv.org/abs/2205.13943"> A2MIM</a></i>, is accepted by <b>ICML 2023</b>. </li>
    <li><b> <font color="#b80000">[2023.04]</font> </b> Co-authored paper on <i>GNN explanation</i>, <i><a href="https://arxiv.org/abs/2301.02780"> MatchExplainer</a></i>, is accepted by <b>ICML 2023</b>, congrats to <a href="https://scholar.google.com/citations?user=8vHkc5YAAAAJ&hl=en" target="_blank" style="color:#2a7ce0">Fang Wu</a>. </li>
    <li><b> <font color="#b80000">[2023.03]</font> </b> Co-authored paper on <i>sign language</i>, <i><a href="https://arxiv.org/abs/2303.05725"> CVT-SLR</a></i>, is accepted by <b>CVPR 2023 (<font color="#FF0000">Highlight</font>)</b>, congrats to <a href="https://scholar.google.com/citations?user=egz8bGQAAAAJ&hl=en" target="_blank" style="color:#2a7ce0">Zheng Jiangbin</a>. </li>
    <li><b> <font color="#b80000">[2023.03]</font> </b> Co-authored paper on <i>video prediction</i>, <i><a href="https://arxiv.org/abs/2206.12126"> TAU</a></i>, is accepted by <b>CVPR 2023</b>, congrats to <a href="https://scholar.google.com/citations?user=6kTV6aMAAAAJ&hl=zh-CN" target="_blank" style="color:#2a7ce0">Cheng Tan</a>. </li>
    <li><b> <font color="#b80000">[2023.01]</font> </b> Co-authored paper on <i>molecular graph pre-training</i>, <i><a href="https://openreview.net/forum?id=jevY-DtiZTR"> Mole-BERT</a></i>, is accepted by <b>ICLR 2023</b>, congrats to <a href="https://scholar.google.com/citations?user=aPKKpSYAAAAJ&hl=en" target="_blank" style="color:#2a7ce0">Jun Xia</a>. </li>
    <li><b> <font color="#b80000">[2022.09]</font> </b> One paper on <i>fine-grained self-supervised learning</i>, <i><a href="https://arxiv.org/abs/2106.15788"> CVSA</a></i>, is accepted by <b>BMVC 2022 (<font color="#FF0000">Spotlight</font>)</b>. </li>
    <li><b> <font color="#b80000">[2022.09]</font> </b> Co-authored paper on <i>domain adaptation</i>, <i><a href="https://nips.cc/Conferences/2022/Schedule?showEvent=55405"> DaC</a></i>, is accepted by <b>NeurIPS 2022</b>, congrats to <a href="https://openreview.net/profile?id=~Ziyi_Zhang2" target="_blank" style="color:#2a7ce0">Ziyi Zhang</a>. </li>
    <li><b> <font color="#b80000">[2022.09]</font> </b> Co-authored paper on <i>graph representation</i>, <i><a href="https://arxiv.org/abs/2104.13048"> DMAGE</a></i>, is accepted by <b>Neurocomputing 2022</b>, congrats to <a href="https://scholar.google.com/citations?user=foERjnQAAAAJ&hl=zh-CN" target="_blank" style="color:#2a7ce0">Zelin Zang</a>. </li>
    <li><b> <font color="#b80000">[2022.08]</font> </b> Co-authored paper on <i>graph attack</i>, <i><a href="https://arxiv.org/abs/2208.05514"> AtkSE</a></i>, is accepted by <b>CIKM 2022</b>, congrats to <a href="https://scholar.google.com/citations?user=OgIdbfAAAAAJ&hl=zh-CN" target="_blank" style="color:#2a7ce0">Zihan Liu</a>. </li>
    <li><b> <font color="#b80000">[2022.07]</font> </b> One paper on <i>data augmentation</i>, <i><a href="https://arxiv.org/abs/2103.13027"> AutoMix</a></i>, is accepted by <b>ECCV 2022 (<font color="#FF0000">Oral</font>)</b>. </li>
    <li><b> <font color="#b80000">[2022.07]</font> </b> One paper on <i>self-supervised learning</i>, <i><a href="https://arxiv.org/abs/2207.03160"> DLME</a></i>, is accepted by <b>ECCV 2022</b>. </li>
    <li><b> <font color="#b80000">[2022.01]</font> </b> Build an open-source toolbox, <i><a href="https://github.com/Westlake-AI/OpenBioSeq"> OpenBioSeq</a></i>, for supervised and self-supervised bio-sequence representation learning based on PyTorch. On updating! </li>
    <li><b> <font color="#b80000">[2022.03]</font> </b> Co-authored paper on <i>semi-supervised learning</i>, <i><a href="https://arxiv.org/abs/2206.00845"> HCR</a></i>, is accepted by <b>CVPR 2022</b>, congrats to <a href="https://scholar.google.com/citations?user=6kTV6aMAAAAJ&hl=zh-CN" target="_blank" style="color:#2a7ce0">Cheng Tan</a>. </li>
    <li><b> <font color="#b80000">[2022.01]</font> </b> Build an open-source toolbox, <i><a href="https://github.com/Westlake-AI/openmixup"> OpenMixup</a></i>, for supervised, self- and semi-supervised visual representation learning based on PyTorch. On updating! </li>
    <li><b> <font color="#b80000">[2021.10]</font> </b> Co-authored paper on <i>deep clustering</i>, <i><a href="https://openaccess.thecvf.com/content/WACV2022/papers/Wu_Generalized_Clustering_and_Multi-Manifold_Learning_With_Geometric_Structure_Preservation_WACV_2022_paper.pdf"> GCML</a></i>, is accepted by <b>WACV 2022</b>, congrats to <a href="https://scholar.google.com/citations?user=Tk7TrCoAAAAJ&hl=zh-CN" target="_blank" style="color:#2a7ce0">Lirong Wu</a>. </li>
    <li><b> <font color="#b80000">[2021.07]</font> </b> Got my B.S. degree from Nanjing University! </li>
    <li><b> <font color="#b80000">[2021.06]</font> </b> One paper on <i>invertible learning</i>, <i><a href="https://arxiv.org/pdf/2010.04012"> inv-ML</a></i>, is accepted by <b>ECML-PKDD 2021</b>. </li>
    <li><b> <font color="#b80000">[2020.05]</font> </b> One paper on <i>visual object tracking</i>, <i><a href="https://www.ijcai.org/proceedings/2020/0099.pdf"> TLPG-Tracker</a></i>, is accepted by <b>IJCAI 2020</b>. </li>
  </ul>
</div>


<A NAME="Interest"><h2>Research Interest</h2></A>
Currently, I focus on Self-supervised Learning and Data-efficient Deep Learning:
<ul>
    <li>Self-supervised Learning in Computer Vision and Bioengineering</li>
    <li>Network Architecture and Long-sequence Modeling</li>
    <li>General Data Augmentation</li>
    <li>Semi-supervised Learning</li>
</ul>
<br />


<A NAME="Education"><h2>Education</h2></A>
<ul>
<li>2021.09-2026.06 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Ph.D at <a href="https://www.zju.edu.cn/" target="_blank" style="color:#2a7ce0">Zhejiang University</a> & <a href="https://www.westlake.edu.cn/" target="_blank" style="color:#2a7ce0">Westlake University</a>. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Supervisor: Prof. <a href="https://scholar.google.com/citations?hl=zh-CN&user=Y-nyLGIAAAAJ" target="_blank" style="color:#2a7ce0">Stan Z. Li</a></li>
<li>2017.09-2021.06 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; B.Sc. at <a href="https://www.nju.edu.cn/" target="_blank" style="color:#2a7ce0">Nanjing University</a>. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Supervisor: Prof. <a href="https://scholar.google.co.id/citations?hl=zh-CN&user=0JRtCV4AAAAJ" target="_blank" style="color:#2a7ce0">Jianxing Wu</a></li>
</ul>
<br />

<A NAME="Internship"><h2>Internship</h2></A>
<ul>
<li>2024.12-2025.12 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Research on AI4Science at <a href="https://www.biomap.com/" target="_blank" style="color:#2a7ce0">BioMap Research</a>, supervised by Researcher <a href="https://openreview.net/profile?id=~Qirong_Yang2" target="_blank" style="color:#2a7ce0">Qirong Yang</a>.</li>
<li>2023.07-2024.08 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Research on AIGC at <a href="https://damo.alibaba.com/" target="_blank" style="color:#2a7ce0">Alibaba DAMO Academy</a>, supervised by Researcher <a href="https://scholar.google.co.id/citations?hl=zh-CN&user=ZNhTHywAAAAJ" target="_blank" style="color:#2a7ce0">Baigui Sun</a>.</li>
<!-- <li>2020.06-2021.08 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Research on Dimension Reduction at <a href="https://www.westlake.edu.cn/" target="_blank" style="color:#2a7ce0">Westlake University</a>, supervised by Prof. <a href="https://scholar.google.com/citations?hl=zh-CN&user=Y-nyLGIAAAAJ" target="_blank" style="color:#2a7ce0">Stan Z. Li</a>.</li> -->
</ul>
<br /> 


<A NAME="Publications"><h2>Publications</h2></A>

<p><b>Selected Preprints</b>: </p>
<font size="3"> 
<ul>

<table class="imgtable"><tr><td>
    <!-- <img src="https://github.com/user-attachments/assets/e9d9af38-d95a-4f76-8a65-574b562da184" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td> -->
    <img src="./imgs/arXiv2024_BOCB_teaser.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2401.00897" target="_blank" style="color:#2a7ce0">Unveiling the Backbone-Optimizer Coupling Bias in Visual Representation Learning</a></b></font><br>
        <i> <b>Siyuan Li</b>*, Juanxi Tian*, Zedong Wang*, Luyuan Zhang, Zicheng Liu, Weiyang Jin, Yang Liu, Baigui Sun, Stan Z. Li </a></i><br><i><b>arXiv, 2024</b></i><br>
        [<a href="https://arxiv.org/abs/2410.06373" target="_blank" style="color:#2a7ce0">PDF</a>]
        [<a href="https://bocb-ai.github.io/" target="_blank" style="color:#2a7ce0">Project</a>]
        [<a href="https://github.com/Black-Box-Optimization-Coupling-Bias/BOCB" target="_blank" style="color:#2a7ce0">Code</a>]
        [<a href="https://zhuanlan.zhihu.com/p/915122368" target="_blank" style="color:#2a7ce0">Zhihu</a>]
        [<a href="./Files/arXiv_2024_BOCB_bibtex" target="_blank" style="color:#2a7ce0">BibTeX</a>]
</p></td></tr></table>

<!-- <table class="imgtable"><tr><td>
    <img src="https://github.com/Westlake-AI/openmixup/assets/44519745/0a30de30-60c4-4479-b17d-093c476c4024" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2402.09240" target="_blank" style="color:#2a7ce0">Switch EMA: A Free Lunch for Better Flatness and Sharpness</a></b></font><br>
        <i> <b>Siyuan Li</b>*, Zicheng Liu*, Juanxi Tian*, Ge Wang*, Zedong Wang, Weiyang Jin, Di Wu, Cheng Tan, Tao Lin, Yang Liu, Baigui Sun, Stan Z. Li </a></i><br><i><b>arXiv, 2024</b></i><br>
        [<a href="https://arxiv.org/abs/2402.09240" target="_blank" style="color:#2a7ce0">PDF</a>]
        [<a href="https://github.com/Westlake-AI/SEMA" target="_blank" style="color:#2a7ce0">Code</a>]
        [<a href="./Files/arXiv_2024_SEMA_Survey_bibtex" target="_blank" style="color:#2a7ce0">BibTeX</a>]
</p></td></tr></table> -->

<!-- <table class="imgtable"><tr><td>
    <img src="https://github.com/Lupin1998/Awesome-MIM/assets/44519745/ec3f11eb-b12d-4ebc-a129-fc951018ddcd" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2401.00897" target="_blank" style="color:#2a7ce0">Masked Modeling for Self-supervised Representation Learning on Vision and Beyond</a></b></font><br>
        <i> <b>Siyuan Li</b>*, Luyuan Zhang*, Zedong Wang, Di Wu, Lirong Wu, Zicheng Liu, Jun Xia, Cheng Tan, Yang Liu, Baigui Sun, Stan Z. Li </a></i><br><i><b>arXiv, 2023</b></i><br>
        [<a href="https://arxiv.org/abs/2401.00897" target="_blank" style="color:#2a7ce0">PDF</a>]
        [<a href="https://github.com/Lupin1998/Awesome-MIM" target="_blank" style="color:#2a7ce0">Code</a>]
        [<a href="https://zhuanlan.zhihu.com/p/677763681" target="_blank" style="color:#2a7ce0">Zhihu</a>]
        [<a href="./Files/arXiv_2023_MIM_Survey_bibtex" target="_blank" style="color:#2a7ce0">BibTeX</a>]
</p></td></tr></table> -->

</ul>
<br />


<p><b>Conferences (As First Author)</b>: </p>
<font size="3"> 
<ul>

<table class="imgtable"><tr><td>
    <img src="./imgs/ACL2025_SGG_teaser.PNG" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2506.01049" target="_blank" style="color:#2a7ce0">Taming LLMs by Scaling Learning Rates with Gradient Grouping</a></b></font><br>
        <i> <b>Siyuan Li</b>*, Juanxi Tian*, Zedong Wang*, Xin Jin, Zicheng Liu, Wentao Zhang, Dan Xu </a></i><br><i><b>Annual Meeting of the Association for Computational Linguistics (ACL), 2025</b></i><br>
        [<a href="https://arxiv.org/abs/2506.01049" target="_blank" style="color:#2a7ce0">PDF</a>]
        [<a href="https://github.com/ScalingOpt/SGG" target="_blank" style="color:#2a7ce0">Code</a>]
        <!-- [<a href="https://zhuanlan.zhihu.com/p/1890772035279504736" target="_blank" style="color:#2a7ce0">Zhihu</a>] -->
        [<a href="https://huggingface.co/papers/2506.01049" target="_blank" style="color:#2a7ce0">HF Paper</a>]
        [<a href="./Files/ACL_2025_SGG_bibtex" target="_blank" style="color:#2a7ce0">BibTeX</a>]
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="./imgs/CVPR2025_MergeVQ_teaser.PNG" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2504.00999" target="_blank" style="color:#2a7ce0">MergeVQ: A Unified Framework for Visual Generation and Representation with Disentangled Token Merging and Quantization</a></b></font><br>
        <i> <b>Siyuan Li</b>*, Luyuan Zhang*, Zedong Wang, Juanxi Tian, Cheng Tan, Zicheng Liu, Chang Yu, Qingsong Xie, Haonan Lu, Haoqian Wang, Zhen Lei </a></i><br><i><b>Conference on Computer Vision and Pattern Recognition (CVPR), 2025</b></i><br>
        [<a href="https://arxiv.org/abs/2504.00999" target="_blank" style="color:#2a7ce0">PDF</a>]
        [<a href="https://apexgen-x.github.io/MergeVQ/" target="_blank" style="color:#2a7ce0">Project</a>]
        [<a href="https://github.com/ApexGen-X/MergeVQ" target="_blank" style="color:#2a7ce0">Code</a>]
        [<a href="https://cvpr.thecvf.com/virtual/2025/poster/33318" target="_blank" style="color:#2a7ce0">Virtual</a>]
        [<a href="https://github.com/Lupin1998/Lupin1998.github.io/blob/master/Files/CVPR_2025_MergeVQ_poster.jpg" target="_blank" style="color:#2a7ce0">Poster</a>]
        [<a href="https://zhuanlan.zhihu.com/p/1890772035279504736" target="_blank" style="color:#2a7ce0">Zhihu</a>]
        [<a href="https://huggingface.co/papers/2504.00999" target="_blank" style="color:#2a7ce0">HF Paper</a>]
        [<a href="./Files/CVPR_2025_MergeVQ_bibtex" target="_blank" style="color:#2a7ce0">BibTeX</a>]
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <!-- <img src="https://github.com/Westlake-AI/openmixup/assets/44519745/8c6f9cf5-897b-457c-9670-d5df5e809515" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td> -->
    <img src="./imgs/ICML2024_VQDNA_teaser.jpg" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2405.10812" target="_blank" style="color:#2a7ce0">VQDNA: Unleashing the Power of Vector Quantization for Multi-Species Genomic Sequence Modeling</a></b></font><br>
        <i> <b>Siyuan Li</b>*, Zedong Wang*, Zicheng Liu, Di Wu, Cheng Tan, Jiangbin Zheng, Yufei Huang, Stan Z. Li </a></i><br><i><b>International Conference on Machine Learning (ICML), 2024</b></i><br>
        [<a href="https://arxiv.org/abs/2405.10812" target="_blank" style="color:#2a7ce0">PDF</a>]
        [<a href="https://github.com/Lupin1998/VQDNA" target="_blank" style="color:#2a7ce0">Code</a>]
        [<a href="https://github.com/Lupin1998/Lupin1998.github.io/blob/master/Files/ICML_2024_VQDNA_poster.pdf" target="_blank" style="color:#2a7ce0">Poster</a>]
        [<a href="./Files/ICML_2024_VQDNA_bibtex" target="_blank" style="color:#2a7ce0">BibTeX</a>]
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/273304810-c59589a3-8aff-4788-896c-5e099e73083e.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2310.03013" target="_blank" style="color:#2a7ce0">SemiReward: A General Reward Model for Semi-supervised Learning</a></b></font><br>
        <i> <b>Siyuan Li</b>*, Weiyang Jin*, Zedong Wang, Fang Wu, Zicheng Liu, Cheng Tan, Stan Z. Li </a></i><br><i><b>International Conference on Learning Representations (ICLR), 2024</b></i><br>
        [<a href="https://arxiv.org/abs/2310.03013" target="_blank" style="color:#2a7ce0">PDF</a>]
        [<a href="https://github.com/Westlake-AI/SemiReward" target="_blank" style="color:#2a7ce0">Code</a>]
        [<a href="https://github.com/Lupin1998/Lupin1998.github.io/blob/master/Files/ICLR_2024_SemiReward_poster.jpg" target="_blank" style="color:#2a7ce0">Poster</a>]
        [<a href="https://zhuanlan.zhihu.com/p/680998327" target="_blank" style="color:#2a7ce0">Zhihu</a>]
        [<a href="./Files/ICLR_2024_SemiReward_bibtex" target="_blank" style="color:#2a7ce0">BibTeX</a>]
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="https://user-images.githubusercontent.com/44519745/202308950-00708e25-9ac7-48f0-af12-224d927ac1ae.jpg" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2211.03295" target="_blank" style="color:#2a7ce0">MogaNet: Efficient Multi-order Gated Aggregation Network</a></b></font><br>
        <i> <b>Siyuan Li</b>*, Zedong Wang*, Zicheng Liu, Cheng Tan, Haitao Lin, Di Wu, Zhiyuan Chen, Jiangbin Zheng, Stan Z. Li </a></i><br><i><b>International Conference on Learning Representations (ICLR), 2024</b></i><br>
        [<a href="https://arxiv.org/abs/2211.03295" target="_blank" style="color:#2a7ce0">PDF</a>]
        [<a href="https://github.com/Westlake-AI/MogaNet" target="_blank" style="color:#2a7ce0">Code</a>]
        [<a href="https://github.com/Lupin1998/Lupin1998.github.io/blob/master/Files/ICLR_2024_MogaNet_poster.png" target="_blank" style="color:#2a7ce0">Poster</a>]
        [<a href="https://zhuanlan.zhihu.com/p/582542948" target="_blank" style="color:#2a7ce0">Zhihu</a>]
        [<a href="./Files/ICLR_2024_MogaNet_bibtex" target="_blank" style="color:#2a7ce0">BibTeX</a>]
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/246222226-61e6b8e8-959c-4bb3-a1cd-c994b423de3f.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2306.11249" target="_blank" style="color:#2a7ce0">OpenSTL: A Comprehensive Benchmark of Spatio-Temporal Predictive Learning</a></b></font><br>
        <i> Cheng Tan*, <b>Siyuan Li</b>*, Zhangyang Gao, Wenfei Guan, Zedong Wang, Zicheng Liu, Lirong Wu, Stan Z. Li </a></i><br><i><b>Advances in Neural Information Processing Systems (NeurIPS), 2023</b></i><br>
        [<a href="https://arxiv.org/abs/2306.11249" target="_blank" style="color:#2a7ce0">PDF</a>]
        [<a href="https://github.com/chengtan9907/OpenSTL" target="_blank" style="color:#2a7ce0">Code</a>]
        [<a href="https://github.com/Lupin1998/Lupin1998.github.io/blob/master/Files/NIPS_2023_OpenSTL_poster.png" target="_blank" style="color:#2a7ce0">Poster</a>]
        [<a href="https://neurips.cc/virtual/2023/poster/73674" target="_blank" style="color:#2a7ce0">Virtual</a>]
        [<a href="https://zhuanlan.zhihu.com/p/640271275" target="_blank" style="color:#2a7ce0">Zhihu</a>]
        [<a href="./Files/NIPS_2023_OpenSTL_bibtex" target="_blank" style="color:#2a7ce0">BibTeX</a>]
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/270054840-2e47e7e5-4533-4290-b78c-9fc44c4019e6.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2203.10761" target="_blank" style="color:#2a7ce0">Harnessing Hard Mixed Samples with Decoupled Regularizer</a></b></font><br>
        <i> Zicheng Liu*, <b>Siyuan Li</b>*, Ge Wang, Cheng Tan, Lirong Wu, Stan Z. Li </a></i><br><i><b>Advances in Neural Information Processing Systems (NeurIPS), 2023</b></i><br>
        [<a href="https://arxiv.org/abs/2203.10761" target="_blank" style="color:#2a7ce0">PDF</a>]
        [<a href="https://github.com/Westlake-AI/openmixup" target="_blank" style="color:#2a7ce0">Code</a>]
        [<a href="https://github.com/Lupin1998/Lupin1998.github.io/blob/master/Files/NIPS_2023_Decouple_poster.png" target="_blank" style="color:#2a7ce0">Poster</a>]
        [<a href="https://neurips.cc/virtual/2023/poster/71299" target="_blank" style="color:#2a7ce0">Virtual</a>]
        [<a href="./Files/NIPS_2023_Decouple_bibtex" target="_blank" style="color:#2a7ce0">BibTeX</a>]
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="https://user-images.githubusercontent.com/44519745/234438993-b5a145ab-d345-46ae-9267-25f68379bb62.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2205.13943" target="_blank" style="color:#2a7ce0">Architecture-Agnostic Masked Image Modeling -- From ViT back to CNN</a></b></font><br>
        <i> <b>Siyuan Li</b>*, Di Wu*, Fang Wu, Zelin Zang, Stan Z. Li </a></i><br><i><b>International Conference on Machine Learning (ICML), 2023</b></i><br>
        [<a href="https://arxiv.org/abs/2205.13943" target="_blank" style="color:#2a7ce0">PDF</a>]
        [<a href="https://github.com/Westlake-AI/openmixup" target="_blank" style="color:#2a7ce0">Code</a>]
        [<a href="https://github.com/Westlake-AI/A2MIM" target="_blank" style="color:#2a7ce0">Project</a>]
        [<a href="https://github.com/Lupin1998/Lupin1998.github.io/blob/master/Files/ICML_2023_A2MIM_poster.png" target="_blank" style="color:#2a7ce0">Poster</a>]
        [<a href="https://icml.cc/virtual/2023/poster/24861" target="_blank" style="color:#2a7ce0">Virtual</a>]
        [<a href="https://zhuanlan.zhihu.com/p/522251181" target="_blank" style="color:#2a7ce0">Zhihu</a>]
        [<a href="./Files/ICML_2023_A2MIM_bibtex" target="_blank" style="color:#2a7ce0">BibTeX</a>]
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="https://user-images.githubusercontent.com/44519745/193470435-634caf9f-3282-4573-a43f-838510af4380.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2106.15788" target="_blank" style="color:#2a7ce0">Exploring Localization for Self-supervised Fine-grained Contrastive Learning</a></b></font><br>
        <i> Di Wu*, <b>Siyuan Li</b>*, Zelin Zang, Stan Z. Li </a></i><br><i><b>British Machine Vision Conference (BMVC), 2022 (<font color="#FF0000">Spotlight, rate: 3.9%</font>)</b></i><br>
        [<a href="https://arxiv.org/abs/2106.15788" target="_blank" style="color:#2a7ce0">PDF</a>] 
        [<a href="https://github.com/Westlake-AI/openmixup" target="_blank" style="color:#2a7ce0">Code</a>]
        [<a href="https://bmvc2022.mpi-inf.mpg.de/0268_video.mp4" target="_blank" style="color:#2a7ce0">Video</a>]
        [<a href="https://bmvc2022.mpi-inf.mpg.de/0268_poster.pdf" target="_blank" style="color:#2a7ce0">Poster</a>]
        <!-- [<a href="./Files/BMVC_2022_CVSA_poster.pdf" target="_blank" style="color:#2a7ce0">Poster</a>] -->
        [<a href="./Files/BMVC_2022_CVSA_bibtex" target="_blank" style="color:#2a7ce0">BibTeX</a>]
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="https://user-images.githubusercontent.com/44519745/193470103-2b995bd5-884f-4c7b-b01d-4f8f4ea96db2.jpg" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2103.13027" target="_blank" style="color:#2a7ce0">AutoMix: Unveiling the Power of Mixup for Stronger Classifiers</a></b></font><br>
        <i> Zicheng Liu*, <b>Siyuan Li</b>*, Di Wu, Zihan Liu, Zhiyuan Chen, Lirong Wu, Stan Z. Li </a></i><br><i><b>European Conference on Computer Vision (ECCV), 2022 (<font color="#FF0000">Oral, rate: 2.7%</font>)</b></i><br>
        [<a href="https://arxiv.org/abs/2103.13027" target="_blank" style="color:#2a7ce0">PDF</a>]
        [<a href="https://doi.org/10.1007/978-3-031-20053-3_26" target="_blank" style="color:#2a7ce0">DOI</a>]
        [<a href="https://github.com/Westlake-AI/openmixup" target="_blank" style="color:#2a7ce0">Code</a>]
        <!-- [<a href="https://drive.google.com/file/d/1s-dsXSoez01PYKi4vKUfy7YELa1FwMak/view?usp=sharing" target="_blank" style="color:#2a7ce0">Poster</a>] -->
        [<a href="./Files/ECCV_2022_AutoMix_poster.jpg" target="_blank" style="color:#2a7ce0">Poster</a>]
        [<a href="https://zhuanlan.zhihu.com/p/550300558" target="_blank" style="color:#2a7ce0">Zhihu</a>]
        [<a href="./Files/ECCV_2022_AutoMix_bibtex" target="_blank" style="color:#2a7ce0">BibTeX</a>]
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="https://user-images.githubusercontent.com/44519745/178650889-fbb21364-b8b0-4601-bcfa-f3293908c3fb.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2207.03160" target="_blank" style="color:#2a7ce0">DLME: Deep Local-flatness Manifold Embedding</a></b></font><br>
        <i> Zelin Zang*, <b>Siyuan Li</b>*, Di Wu, Ge Wang, Lei Shang, Baigui Sun, Hao Li, Stan Z. Li </a></i><br><i><b>European Conference on Computer Vision (ECCV), 2022</b></i><br>
        [<a href="https://arxiv.org/abs/2207.03160" target="_blank" style="color:#2a7ce0">PDF</a>]
        [<a href="https://doi.org/10.1007/978-3-031-19803-8_34" target="_blank" style="color:#2a7ce0">DOI</a>]
        [<a href="https://github.com/zangzelin/code_ECCV2022_DLME" target="_blank" style="color:#2a7ce0">Code</a>]
        [<a href="./Files/ECCV_2022_DLME_poster.jpg" target="_blank" style="color:#2a7ce0">Poster</a>]
        [<a href="./Files/ECCV_2022_DLME_bibtex" target="_blank" style="color:#2a7ce0">BibTeX</a>] 
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="https://user-images.githubusercontent.com/44519745/178662955-9addc1e0-467d-4826-8e58-b57d5e40e323.PNG" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/pdf/2010.04012" target="_blank" style="color:#2a7ce0">Invertible Manifold Learning for Dimension Reduction</a></b></font><br>
        <i> <b>Siyuan Li</b>, Haitao Lin, Zelin Zang, Lirong Wu, Jun Xia, Stan Z. Li </a></i><br><i><b>European Conference on Machine Learning (ECML), 2021</b></i><br>
        [<a href="https://arxiv.org/pdf/2010.04012" target="_blank" style="color:#2a7ce0">PDF</a>]
        [<a href="https://doi.org/10.1007/978-3-030-86523-8_43" target="_blank" style="color:#2a7ce0">DOI</a>]
        [<a href="https://github.com/Westlake-AI/inv-ML" target="_blank" style="color:#2a7ce0">Code</a>] 
        [<a href="https://slideslive.com/38963544/invertible-manifold-learning-for-dimension-reduction" target="_blank" style="color:#2a7ce0">Video</a>]
        [<a href="./Files/ECML_2021_INV_poster.jpg" target="_blank" style="color:#2a7ce0">Poster</a>]
        [<a href="./Files/ECML_2021_INV_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] 
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="https://user-images.githubusercontent.com/44519745/178662659-2d24500d-05ff-49b5-a151-22d7d0e00eaf.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://www.ijcai.org/proceedings/2020/0099.pdf" target="_blank" style="color:#2a7ce0">TLPG-Tracker: Joint Learning of Target Localization and Proposal Generation for Visual Tracking</a></b></font><br>
        <i> <b>Siyuan Li</b>, Zhi Zhang, Ziyu Liu, Anna Wang, Linglong Qiu, Feng Du </a></i><br><i><b>International Joint Conference on Artificial Intelligence (IJCAI), 2020</b></i><br>
        [<a href="https://www.ijcai.org/proceedings/2020/0099.pdf" target="_blank" style="color:#2a7ce0">PDF</a>] 
        [<a href="https://github.com/Lupin1998/TLPG_Tracker" target="_blank" style="color:#2a7ce0">Code</a>] 
        [<a href="https://www.ijcai.org/proceedings/2020/video/25319" target="_blank" style="color:#2a7ce0">Video</a>]
        [<a href="./Files/IJCAI_2020_TLPG_poster.jpg" target="_blank" style="color:#2a7ce0">Poster</a>]
        [<a href="./Files/IJCAI_2020_TLPG_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] 
</p></td></tr></table>

</ul>
<br />


<p><b>Conferences (As Co-author)</b>: </p>
<font size="3"> 
<ul>

<table class="imgtable"><tr><td>
    <img src="./imgs/ICCV2025_RepMTL_teaser.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2410.12866" target="_blank" style="color:#2a7ce0">Rep-MTL: Unleashing the Power of Representation-level Task Saliency for Multi-Task Learning</a></b></font><br>
        <i> Zedong Wang, <b>Siyuan Li</b>, and Dan Xu </a></i><br><i><b>International Conference on Computer Vision (ICCV), 2025</b></i><br>
        <!-- [<a href="https://arxiv.org/abs/2410.12866" target="_blank" style="color:#2a7ce0">PDF</a>] -->
        [<a href="./Files/ICCV_2025_RepMTL_bibtex" target="_blank" style="color:#2a7ce0">BibTeX</a>]
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="./imgs/CVPR2025_DiagramGenBenchmark_teaser.PNG" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2411.11916" target="_blank" style="color:#2a7ce0">From Words to Structured Visuals: A Benchmark and Framework for Text-to-Diagram Generation and Editing</a></b></font><br>
        <i> Jingxuan Wei*, Cheng Tan*, Qi Chen*, Gaowei Wu*, <b>Siyuan Li</b>, Zhangyang Gao, Linzhuang Sun, Bihui Yu, and Ruifeng Guo </a></i><br><i><b>Conference on Computer Vision and Pattern Recognition (CVPR), 2025</b></i><br>
        [<a href="https://arxiv.org/abs/2411.11916" target="_blank" style="color:#2a7ce0">PDF</a>]
        [<a href="https://github.com/Lupin1998/Lupin1998.github.io/blob/master/Files/CVPR_2025_DiagramGenBenchmark_poster.jpg" target="_blank" style="color:#2a7ce0">Poster</a>]
        [<a href="./Files/CVPR_2025_DiagramGenBenchmark_bibtex" target="_blank" style="color:#2a7ce0">BibTeX</a>]
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="https://github.com/user-attachments/assets/d81fc11e-6cfa-4549-af4f-c21a65507628" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2410.12866" target="_blank" style="color:#2a7ce0">Towards Homogeneous Lexical Tone Decoding from Heterogeneous Intracranial Recordings</a></b></font><br>
        <i> Di Wu, <b>Siyuan Li</b>, Chen Feng, Lu Cao, Yue Zhang, Jie Yang, and Mohamad Sawan </a></i><br><i><b>International Conference on Learning Representations (ICLR), 2025</b></i><br>
        [<a href="https://arxiv.org/abs/2410.12866" target="_blank" style="color:#2a7ce0">PDF</a>]
        [<a href="./Files/ICLR_2025_H2DiLR_bibtex" target="_blank" style="color:#2a7ce0">BibTeX</a>]
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="https://github.com/user-attachments/assets/e99f7862-7551-4a78-b7ef-8d2022e6a52a" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2503.01136" target="_blank" style="color:#2a7ce0">Prior-guided Hierarchical Harmonization Network for Efficient Image Dehazing</a></b></font><br>
        <i> Xiongfei Su, <b>Siyuan Li</b>, Yuning Cui, Miao Cao, Yulun Zhang, Zheng Chen, Zongliang Wu, Zedong Wang, Yuanlong Zhang, and Xin Yuan </a></i><br><i><b>Annual AAAI Conference on Artificial Intelligence (AAAI), 2025</b></i><br>
        [<a href="https://arxiv.org/abs/2503.01136" target="_blank" style="color:#2a7ce0">PDF</a>]
        [<a href="./Files/AAAI_2025_PGHNet_bibtex" target="_blank" style="color:#2a7ce0">BibTeX</a>]
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="https://github.com/user-attachments/assets/011d6ed1-02f8-4343-8b31-4b92f3f5e76d" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2304.03906" target="_blank" style="color:#2a7ce0">Instructor-inspired Machine Learning for Robust Molecular Property Prediction</a></b></font><br>
        <i> Fang Wu, Shuting Jin, <b>Siyuan Li</b>, and Stan Z. Li </a></i><br><i><b>Advances in Neural Information Processing Systems (NeurIPS), 2024</b></i><br>
        [<a href="https://arxiv.org/abs/2304.03906" target="_blank" style="color:#2a7ce0">PDF</a>]
        [<a href="https://github.com/smiles724/InstructMol" target="_blank" style="color:#2a7ce0">Code</a>] 
        [<a href="./Files/NIPS_2024_InstructBio_bibtex" target="_blank" style="color:#2a7ce0">BibTeX</a>]
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="https://github.com/user-attachments/assets/0fe340f0-fceb-47e3-8393-08e9edb89670" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2412.18827" target="_blank" style="color:#2a7ce0">PhyloGen: Language Model-Enhanced Phylogenetic Inference via Graph Structure Generation</a></b></font><br>
        <i> ChenRui Duan, Zelin Zang, <b>Siyuan Li</b>, Yongjie Xu, and Stan Z. Li </a></i><br><i><b>Advances in Neural Information Processing Systems (NeurIPS), 2024</b></i><br>
        [<a href="https://arxiv.org/abs/2412.18827" target="_blank" style="color:#2a7ce0">PDF</a>]
        [<a href="./Files/NIPS_2024_PhyloGen_bibtex" target="_blank" style="color:#2a7ce0">BibTeX</a>]
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="https://github.com/user-attachments/assets/17ab1c0e-71f7-4a41-8c3a-c37fa14c7ef1" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2410.10587" target="_blank" style="color:#2a7ce0">TopoFR: A Closer Look at Topology Alignment on Face Recognition</a></b></font><br>
        <i> Jun Dan, Yang Liu, Jiankang Deng, Haoyu Xie, <b>Siyuan Li</b>, Baigui Sun, and Shan Luo </a></i><br><i><b>Advances in Neural Information Processing Systems (NeurIPS), 2024</b></i><br>
        [<a href="https://arxiv.org/abs/2410.10587" target="_blank" style="color:#2a7ce0">PDF</a>]
        [<a href="https://github.com/modelscope/facechain/tree/main/face_module/TopoFR" target="_blank" style="color:#2a7ce0">Code</a>] 
        [<a href="./Files/NIPS_2024_TopoFR_bibtex" target="_blank" style="color:#2a7ce0">BibTeX</a>]
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="https://github.com/Westlake-AI/openmixup/assets/44519745/3c929a74-ef1b-4ada-9c1b-7720e9edd8e7" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2311.14109" target="_blank" style="color:#2a7ce0">Boosting the Power of Small Multimodal Reasoning Models to Match Larger Models with Self-Consistency Training</a></b></font><br>
        <i> Cheng Tan, Jingxuan Wei, Zhangyang Gao, Linzhuang Sun, <b>Siyuan Li</b>, Xihong Yang, and Stan Z. Li </a></i><br><i><b>European Conference on Computer Vision (ECCV), 2024</b></i><br>
        [<a href="https://arxiv.org/abs/2311.14109" target="_blank" style="color:#2a7ce0">PDF</a>]
        [<a href="https://github.com/chengtan9907/mc-cot" target="_blank" style="color:#2a7ce0">Code</a>] 
        [<a href="./Files/ICML_2024_MCCoT_bibtex" target="_blank" style="color:#2a7ce0">BibTeX</a>]
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="https://github.com/Westlake-AI/openmixup/assets/44519745/d984b933-d58d-41df-9adc-63cdd208bae8" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2406.08128" target="_blank" style="color:#2a7ce0">Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences</a></b></font><br>
        <i> Zicheng Liu, <b>Siyuan Li</b>, Li Wang, Zedong Wang, Yunfan Liu, and Stan Z. Li </a></i><br><i><b>International Conference on Machine Learning (ICML), 2024</b></i><br>
        [<a href="https://arxiv.org/abs/2406.08128" target="_blank" style="color:#2a7ce0">PDF</a>]
        [<a href="https://github.com/pone7/CHELA" target="_blank" style="color:#2a7ce0">Code</a>] 
        [<a href="./Files/ICML_2024_CHELA_bibtex" target="_blank" style="color:#2a7ce0">BibTeX</a>]
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="https://github.com/Westlake-AI/openmixup/assets/44519745/06e87eb2-b9ac-44ba-a48f-59e6a4c82bd6" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2402.11459" target="_blank" style="color:#2a7ce0">Re-Dock: Towards Flexible and Realistic Molecular Docking with Diffusion Bridge</a></b></font><br>
        <i> Yufei Huang, Odin Zhang, Lirong Wu, Cheng Tan, Haitao Lin, Zhangyang Gao, <b>Siyuan Li</b>, and Stan Z. Li </a></i><br><i><b>International Conference on Machine Learning (ICML), 2024 (<font color="#FF0000">Highlight, rate: 5%</font>)</b></i><br>
        [<a href="https://arxiv.org/abs/2402.11459" target="_blank" style="color:#2a7ce0">PDF</a>]
        [<a href="https://github.com/Lupin1998/Lupin1998.github.io/blob/master/Files/ICML_2024_ReDock_poster.png" target="_blank" style="color:#2a7ce0">Poster</a>]
        [<a href="./Files/ICML_2024_ReDock_bibtex" target="_blank" style="color:#2a7ce0">BibTeX</a>]
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="https://github.com/Westlake-AI/openmixup/assets/44519745/3d1953af-4060-48bc-bf70-250d09148487" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2405.10348" target="_blank" style="color:#2a7ce0">Learning to Predict Mutation Effects of Protein-Protein Interactions by Microenvironment-aware Hierarchical Prompt Learning</a></b></font><br>
        <i> Lirong Wu, Yijun Tian, Haitao Lin, Yufei Huang, <b>Siyuan Li</b>, Nitesh V Chawla, and Stan Z. Li </a></i><br><i><b>International Conference on Machine Learning (ICML), 2024</b></i><br>
        [<a href="https://arxiv.org/abs/2405.10348" target="_blank" style="color:#2a7ce0">PDF</a>]
        [<a href="https://github.com/lirongwu/prompt-ddg" target="_blank" style="color:#2a7ce0">Code</a>]
        [<a href="https://github.com/Lupin1998/Lupin1998.github.io/blob/master/Files/ICML_2024_MMM_poster.png" target="_blank" style="color:#2a7ce0">Poster</a>]
        [<a href="./Files/ICML_2024_MMM_bibtex" target="_blank" style="color:#2a7ce0">BibTeX</a>]
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="https://github.com/Westlake-AI/openmixup/assets/44519745/1ffe9205-2ea8-40c3-87c2-8f8fbac25a32" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2404.11163" target="_blank" style="color:#2a7ce0">LongVQ: Long Sequence Modeling with Vector Quantization on Structured Memory</a></b></font><br>
        <i> Zicheng Liu, Li Wang, <b>Siyuan Li</b>, Zedong Wang, Haitao Lin, and Stan Z. Li </a></i><br><i><b>International Joint Conference on Artificial Intelligence (IJCAI), 2024 (<font color="#FF0000">Oral, rate: 4.0%</font>)</b></i><br>
        [<a href="https://arxiv.org/abs/2404.11163" target="_blank" style="color:#2a7ce0">PDF</a>]
        [<a href="./Files/IJCAI_2024_LongVQ_bibtex" target="_blank" style="color:#2a7ce0">BibTeX</a>]
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="https://github.com/Westlake-AI/openmixup/assets/44519745/c7285dfb-b150-48e6-83c2-3436eed1e307" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://openreview.net/forum?id=itGkF993gz" target="_blank" style="color:#2a7ce0">MAPE-PPI: Towards Effective and Efficient Protein-Protein Interaction Prediction via Microenvironment-Aware Protein Embedding</a></b></font><br>
        <i> Lirong Wu, Yijun Tian, Yufei Huang, <b>Siyuan Li</b>, Haitao Lin, Nitesh V Chawla, and Stan Z. Li </a></i><br><i><b>International Conference on Learning Representations (ICLR), 2024 (<font color="#FF0000">Highlight, rate: 5%</font>)</b></i><br>
        [<a href="https://openreview.net/forum?id=itGkF993gz" target="_blank" style="color:#2a7ce0">PDF</a>]
        [<a href="https://github.com/LirongWu/Homophily-Enhanced-Self-supervision" target="_blank" style="color:#2a7ce0">Code</a>] 
        [<a href="./Files/ICLR_2024_MAPEPPI_bibtex" target="_blank" style="color:#2a7ce0">BibTeX</a>]
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="https://github.com/Westlake-AI/openmixup/assets/44519745/905010ea-4953-4d67-a60b-f13c66a02b83" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2301.10774" target="_blank" style="color:#2a7ce0">Hierarchical Data-efficient Representation Learning for Tertiary Structure-based RNA Design</a></b></font><br>
        <i> Cheng Tan, Yijie Zhang, Zhangyang Gao, Bozhen Hu, <b>Siyuan Li</b>, Zicheng Liu, and Stan Z. Li </a></i><br><i><b>International Conference on Learning Representations (ICLR), 2024</b></i><br>
        [<a href="https://arxiv.org/abs/2301.10774" target="_blank" style="color:#2a7ce0">PDF</a>]
        [<a href="https://github.com/A4Bio/RDesign" target="_blank" style="color:#2a7ce0">Code</a>] 
        [<a href="./Files/ICLR_2024_RDesign_bibtex" target="_blank" style="color:#2a7ce0">BibTeX</a>]
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="https://github.com/Westlake-AI/openmixup/assets/44519745/363033f5-6e13-46a4-8656-a66895f523d6" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://openreview.net/forum?id=mpqMVWgqjn" target="_blank" style="color:#2a7ce0">KW-Design: Pushing the Limit of Protein Deign via Knowledge Refinement</a></b></font><br>
        <i> Zhangyang Gao, Cheng Tan, Xingran Chen, Yijie Zhang, Jun Xia, <b>Siyuan Li</b>, and Stan Z. Li </a></i><br><i><b>International Conference on Learning Representations (ICLR), 2024</b></i><br>
        [<a href="https://openreview.net/forum?id=mpqMVWgqjn" target="_blank" style="color:#2a7ce0">PDF</a>]
        [<a href="https://github.com/A4Bio/OpenCPD" target="_blank" style="color:#2a7ce0">Code</a>] 
        [<a href="./Files/ICLR_2024_KWDesign_bibtex" target="_blank" style="color:#2a7ce0">BibTeX</a>]
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="https://github.com/Westlake-AI/Awesome-Mixup/assets/44519745/ae539c07-927e-4618-b3de-361e55229778" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2310.11466" target="_blank" style="color:#2a7ce0">Protein 3D Graph Structure Learning for Robust Structure-based Protein Property Prediction</a></b></font><br>
        <i> Yufei Huang, <b>Siyuan Li</b>, Jin Su, Lirong Wu, Odin Zhang, Haitao Lin, Jingqi Qi, Zihan Liu, Zhangyang Gao, Yuyang Liu, Jiangbin Zheng, and Stan Z. Li </a></i><br><i><b>Annual AAAI Conference on Artificial Intelligence (AAAI), 2024</b></i><br>
        [<a href= "https://arxiv.org/abs/2310.11466" target="_blank" style="color:#2a7ce0">PDF</a>]
        [<a href="https://underline.io/lecture/93047-protein-3d-graph-structure-learning-for-robust-structure-based-protein-property-prediction" target="_blank" style="color:#2a7ce0">Video</a>]
        [<a href="./Files/AAAI_2024_SAO_bibtex" target="_blank" style="color:#2a7ce0">BibTeX</a>]
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="https://github.com/chengtan9907/OpenSTL/assets/44519745/534bc177-0c93-4aa9-ba04-eb71a641bfe0" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://ojs.aaai.org/index.php/AAAI/article/view/28230" target="_blank" style="color:#2a7ce0">Wavelet-Driven Spatiotemporal Predictive Learning: Bridging Frequency and Time Variations</a></b></font><br>
        <i> Xuesong Nie, Yunfeng Yan, <b>Siyuan Li</b>, Cheng Tan, Xi Chen, Haoyuan Jin, Zhihang Zhu, Stan Z. Li, and Donglian Qi </a></i><br><i><b>Annual AAAI Conference on Artificial Intelligence (AAAI), 2024</b></i><br>
        [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/28230" target="_blank" style="color:#2a7ce0">PDF</a>]
        [<a href="https://underline.io/lecture/91861-wavelet-driven-spatiotemporal-predictive-learning-bridging-frequency-and-time-variations" target="_blank" style="color:#2a7ce0">Video</a>]
        [<a href="./Files/AAAI_2024_WaST_bibtex" target="_blank" style="color:#2a7ce0">BibTeX</a>]
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/270491609-b56802ff-1bd2-44eb-805a-0e2efcc76f3e.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2306.13769" target="_blank" style="color:#2a7ce0">Functional-Group-Based Diffusion for Pocket-Specific Molecule Generation and Elaboration</a></b></font><br>
        <i> Haitao Lin, Yufei Huang, Haotian Zhang, Lirong Wu, <b>Siyuan Li</b>, Zhiyuan Chen, Stan Z. Li </a></i><br><i><b>Advances in Neural Information Processing Systems (NeurIPS), 2023</b></i><br>
        [<a href="https://arxiv.org/abs/2306.13769" target="_blank" style="color:#2a7ce0">PDF</a>]
        [<a href="https://github.com/edapinenut/cbgbench" target="_blank" style="color:#2a7ce0">Code</a>]
        [<a href="https://neurips.cc/virtual/2023/poster/70596" target="_blank" style="color:#2a7ce0">Virtual</a>]
        [<a href="./Files/NIPS_2023_D3FG_bibtex" target="_blank" style="color:#2a7ce0">BibTeX</a>]
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="https://user-images.githubusercontent.com/44519745/235269631-4a392211-8e97-4582-a662-a5a28c9c8bed.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2303.05725" target="_blank" style="color:#2a7ce0">Explaining Graph Neural Networks via Non-parametric Subgraph Matching</a></b></font><br>
        <i> Fang Wu, <b>Siyuan Li</b>, Xurui Jin, Yinghui Jiang, Dragomir Radev, Zhangming Niu, Stan Z. Li </a></i><br><i><b>International Conference on Machine Learning (ICML), 2023</b></i><br>
        [<a href="https://arxiv.org/abs/2301.02780" target="_blank" style="color:#2a7ce0">PDF</a>]
        [<a href="https://github.com/smiles724/MatchExplainer" target="_blank" style="color:#2a7ce0">Code</a>]
        [<a href="https://github.com/Lupin1998/Lupin1998.github.io/blob/master/Files/ICML_2023_MatchExplainer_poster.png" target="_blank" style="color:#2a7ce0">Poster</a>]
        [<a href="https://icml.cc/virtual/2023/poster/25214" target="_blank" style="color:#2a7ce0">Virtual</a>]
        [<a href="./Files/ICML_2023_MatchExplainer_bibtex" target="_blank" style="color:#2a7ce0">BibTeX</a>]
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="https://user-images.githubusercontent.com/44519745/225110366-253c39c6-f587-42ec-bb8b-f5115fd294d1.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2303.05725" target="_blank" style="color:#2a7ce0">CVT-SLR: Contrastive Visual-Textual Transformation for Sign Language Recognition with Variational Alignment</a></b></font><br>
        <i> Jiangbin Zheng, Yile Wang, Cheng Tan, <b>Siyuan Li</b>, Ge Wang, Jun Xia, Yidong Chen, Stan Z. Li </a></i><br><i><b>Computer Vision and Pattern Recognition (CVPR), 2023 (<font color="#FF0000">Highlight, rate: 2.5%</font>)</b></i><br>
        [<a href="https://arxiv.org/abs/2303.05725" target="_blank" style="color:#2a7ce0">PDF</a>]
        [<a href="https://github.com/binbinjiang/CVT-SLR" target="_blank" style="color:#2a7ce0">Code</a>]
        [<a href="./Files/CVPR_2023_CVTSLR_bibtex" target="_blank" style="color:#2a7ce0">BibTeX</a>]
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="https://user-images.githubusercontent.com/44519745/225091680-8bddc78e-6fe9-4c04-ab73-d67106d9ea18.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2206.12126" target="_blank" style="color:#2a7ce0">Temporal attention unit: Towards efficient spatiotemporal predictive learning</a></b></font><br>
        <i> Cheng Tan*, Zhangyang Gao*, Lirong Wu, Yongjie Xu, Jun Xia, <b>Siyuan Li</b>, Stan Z. Li </a></i><br><i><b>Computer Vision and Pattern Recognition (CVPR), 2023</b></i><br>
        [<a href="https://arxiv.org/abs/2206.12126" target="_blank" style="color:#2a7ce0">PDF</a>]
        [<a href="https://github.com/chengtan9907/SimVPv2" target="_blank" style="color:#2a7ce0">Code</a>]
        [<a href="./Files/CVPR_2023_TAU_bibtex" target="_blank" style="color:#2a7ce0">BibTeX</a>]
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="https://user-images.githubusercontent.com/44519745/217610694-89d45be9-c957-4a52-85b2-197b9aab753d.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://openreview.net/forum?id=jevY-DtiZTR" target="_blank" style="color:#2a7ce0">Mole-BERT: Rethinking Pre-training Graph Neural Networks for Molecules</a></b></font><br>
        <i> Jun Xia, Chengshuai Zhao, Bozhen Hu, Zhangyang Gao, Cheng Tan, Yue Liu, <b>Siyuan Li</b>, Stan Z. Li </a></i><br><i><b>International Conference on Learning Representations (ICLR), 2023</b></i><br>
        [<a href= "https://openreview.net/forum?id=jevY-DtiZTR" target="_blank" style="color:#2a7ce0">PDF</a>]
        [<a href="https://github.com/junxia97/Mole-BERT" target="_blank" style="color:#2a7ce0">Code</a>]
        [<a href="./Files/ICLR_2023_MoleBERT_bibtex" target="_blank" style="color:#2a7ce0">BibTeX</a>]
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="https://user-images.githubusercontent.com/44519745/193472721-8d8a19a6-12b0-4e01-ac7b-5c3f87220dc1.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://openreview.net/forum?id=NjImFaBEHl" target="_blank" style="color:#2a7ce0">Divide and Contrast: Source-free Domain Adaptation via Adaptive Contrastive Learning</a></b></font><br>
        <i> Ziyi Zhang, Weikai Chen, Hui Cheng, Zhen Li, <b>Siyuan Li</b>, Liang Lin, Guanbin Li </a></i><br><i><b>Advances in Neural Information Processing Systems (NeurIPS), 2022</b></i><br>
        [<a href="https://nips.cc/Conferences/2022/Schedule?showEvent=55405" target="_blank" style="color:#2a7ce0">PDF</a>] 
        [<a href="https://github.com/ZyeZhang/DaC" target="_blank" style="color:#2a7ce0">Code</a>]
        [<a href="./Files/NIPS_2022_DaC_bibtex" target="_blank" style="color:#2a7ce0">BibTeX</a>] 
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="https://user-images.githubusercontent.com/44519745/185804286-054b1a5c-39db-483a-94a5-55afd70ea7ca.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2208.05514" target="_blank" style="color:#2a7ce0">Are Gradients on Graph Structure Reliable in Gray-box Attacks?</a></b></font><br>
        <i> Zihan Liu, Yun Luo, Lirong Wu, <b>Siyuan Li</b>, Zicheng Liu, Stan Z. Li </a></i><br><i><b>The Conference on Information and Knowledge Management (CIKM), 2022</b></i><br>
        [<a href= "https://arxiv.org/abs/2208.05514" target="_blank" style="color:#2a7ce0">PDF</a>] 
        [<a href="https://github.com/zihan-liu-00/atkse" target="_blank" style="color:#2a7ce0">Code</a>]
        [<a href="./Files/CIKM_2022_AtkSE_bibtex" target="_blank" style="color:#2a7ce0">BibTeX</a>] 
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="https://user-images.githubusercontent.com/44519745/178664546-e9c532b9-297b-4875-b5ac-332d24a14007.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2206.00845" target="_blank" style="color:#2a7ce0">Hyperspherical Consistency Regularization</a></b></font><br>
        <i> Cheng Tan*, Zhangyang Gao*, Lirong Wu, <b>Siyuan Li</b>, Stan Z. Li </a></i><br><i><b>Computer Vision and Pattern Recognition (CVPR), 2022</b></i><br>
        [<a href= "https://arxiv.org/abs/2206.00845" target="_blank" style="color:#2a7ce0">PDF</a>] 
        [<a href="https://github.com/chengtan9907/Hyperspherical-Consistency-Regularization" target="_blank" style="color:#2a7ce0">Code</a>]
        [<a href="./Files/CVPR_2022_HCR_bibtex" target="_blank" style="color:#2a7ce0">BibTeX</a>] 
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="https://user-images.githubusercontent.com/44519745/178662887-94b47cce-d235-4cad-8284-a5e14518e2cf.PNG" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://openaccess.thecvf.com/content/WACV2022/papers/Wu_Generalized_Clustering_and_Multi-Manifold_Learning_With_Geometric_Structure_Preservation_WACV_2022_paper.pdf" target="_blank" style="color:#2a7ce0">Generalized Clustering and Multi-Manifold Learning with Geometric Structure Preservation</a></b></font><br>
        <i>Lirong Wu, Zicheng Liu, Zelin Zang, Jun Xia, <b>Siyuan Li</b>, Stan Z. Li </a></i><br><i><b>Winter Conference on Applications of Computer Vision (WACV), 2022</b></i><br>
        [<a href= "https://openaccess.thecvf.com/content/WACV2022/papers/Wu_Generalized_Clustering_and_Multi-Manifold_Learning_With_Geometric_Structure_Preservation_WACV_2022_paper.pdf" target="_blank" style="color:#2a7ce0">PDF</a>] 
        [<a href="https://github.com/LirongWu/GCML" target="_blank" style="color:#2a7ce0">Code</a>] 
        [<a href="./Files/WACV_2022_GCML_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] 
</p></td></tr></table>

</ul>
<br />


<p><b>Journals</b>: </p>
<font size="3"> 
<ul>

<table class="imgtable"><tr><td>
    <img src="https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/279520065-c821a9c3-b131-445d-9d28-06d80246e42a.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2110.14553" target="_blank" style="color:#2a7ce0">GenURL: A General Framework for Unsupervised Representation Learning</a></b></font><br>
        <i> <b>Siyuan Li</b>*, Zicheng Liu*, Zelin Zang, Di Wu, Zhiyuan Chen, Stan Z. Li </a></i><br><i><b>IEEE Transactions on Neural Networks and Learning Systems (TNNLS), 2024</b></i><br>
        [<a href="https://arxiv.org/abs/2110.14553" target="_blank" style="color:#2a7ce0">PDF</a>]
        [<a href="https://doi.org/10.1109/TNNLS.2023.3332087" target="_blank" style="color:#2a7ce0">DOI</a>] 
        [<a href="https://github.com/Westlake-AI/openmixup" target="_blank" style="color:#2a7ce0">Code</a>] 
        [<a href="./Files/TNNLS_2023_GenURL_bibtex" target="_blank" style="color:#2a7ce0">BibTeX</a>] 
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="https://github.com/user-attachments/assets/a387bf8e-7a5e-44dd-b28c-a0bca511b0b1" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2205.07266" target="_blank" style="color:#2a7ce0">Discovering the Representation Bottleneck of Graph Neural Networks</a></b></font><br>
        <i> Fang Wu*, <b>Siyuan Li</b>*, Stan Z. Li </a></i><br><i><b>IEEE Transactions on Knowledge and Data Engineering (TKDE), 2024</b></i><br>
        [<a href="https://arxiv.org/abs/2205.07266" target="_blank" style="color:#2a7ce0">PDF</a>]
        [<a href="https://doi.org/10.1109/TKDE.2024.3446584" target="_blank" style="color:#2a7ce0">DOI</a>] 
        [<a href="https://github.com/smiles724/GNN-Bottleneck" target="_blank" style="color:#2a7ce0">Code</a>] 
        [<a href="./Files/TKDE_2024_GNNBottleneck_bibtex" target="_blank" style="color:#2a7ce0">BibTeX</a>] 
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="https://github.com/Westlake-AI/OpenBioSeq/assets/44519745/5fc982e2-d4b1-49f3-96f3-c077329c6a2f" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2204.12440" target="_blank" style="color:#2a7ce0">Neuro-BERT: Rethinking Masked Autoencoding for Self-Supervised Neurological Pretraining</a></b></font><br>
        <i> Di Wu, <b>Siyuan Li</b>, Jie Yang, Mohamad Sawan </a></i><br><i><b>IEEE Journal of Biomedical and Health Informatics (JBHI), 2024</b></i><br>
        [<a href="https://arxiv.org/abs/2204.12440" target="_blank" style="color:#2a7ce0">PDF</a>]
        [<a href="https://doi.org/10.1109/JBHI.2024.3415959" target="_blank" style="color:#2a7ce0">DOI</a>] 
        [<a href="https://github.com/Westlake-AI/OpenBioSeq" target="_blank" style="color:#2a7ce0">Code</a>] 
        [<a href="./Files/JBHI_2024_NeuroBERT_bibtex" target="_blank" style="color:#2a7ce0">BibTeX</a>] 
</p></td></tr></table>

<!-- <table class="imgtable"><tr><td>
    <img src="https://github.com/Westlake-AI/openmixup/assets/44519745/6d72f88a-df44-42b6-87e4-4a436bd87fb8" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2307.12626" target="_blank" style="color:#2a7ce0">Enhancing Human-like Multi-Modal Reasoning: A New Challenging Dataset and Comprehensive Framework</a></b></font><br>
        <i> Jingxuan Wei, Cheng Tan, Zhangyang Gao, Linzhuang Sun, <b>Siyuan Li</b>, Bihui Yu, Ruifeng Guo, Stan Z. Li </a></i><br><i><b>Neural Computing & Applications (NCAA), 2023</b></i><br>
        [<a href="https://arxiv.org/abs/2307.12626" target="_blank" style="color:#2a7ce0">PDF</a>]
        [<a href="https://github.com/weijingxuan/COCO-MMR" target="_blank" style="color:#2a7ce0">Code</a>] 
        [<a href="./Files/NCAA_2024_MMR_bibtex" target="_blank" style="color:#2a7ce0">BibTeX</a>] 
</p></td></tr></table> -->

<table class="imgtable"><tr><td>
    <img src="https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/254981332-1a4d2d1d-afda-45f7-b8e9-504d7a07c99d.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://ieeexplore.ieee.org/abstract/document/10158394" target="_blank" style="color:#2a7ce0">GNN Cleaner: Label Cleaner for Graph-structured Data</a></b></font><br>
        <i> Jun Xia, Haitao Lin, Yongjie Xu, Cheng Tan, Lirong Wu, <b>Siyuan Li</b>, Stan Z. Li </a></i><br><i><b>IEEE Transactions on Knowledge and Data Engineering (TKDE), 2023</b></i><br>
        [<a href= "https://ieeexplore.ieee.org/abstract/document/10158394" target="_blank" style="color:#2a7ce0">PDF</a>] 
        [<a href="./Files/TKDE_2023_GNNCleaner_bibtex" target="_blank" style="color:#2a7ce0">BibTeX</a>] 
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="https://user-images.githubusercontent.com/44519745/189410755-01edbd98-0cd8-4f18-a19e-abb46dc9508d.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/abs/2104.13048" target="_blank" style="color:#2a7ce0">Deep Manifold Attributed Graph Embedding</a></b></font><br>
        <i> Zelin Zang, <b>Siyuan Li</b>, Di Wu, Jianzhu Guo, Yongjie Xu, Stan Z. Li </a></i><br><i><b>Neurocomputing, 2022</b></i><br>
        [<a href= "https://arxiv.org/abs/2104.13048" target="_blank" style="color:#2a7ce0">PDF</a>] 
        [<a href="https://github.com/zangzelin/paper_code_DMAGE" target="_blank" style="color:#2a7ce0">Code</a>] 
        [<a href="./Files/NC_2022_DMAGE_bibtex" target="_blank" style="color:#2a7ce0">BibTeX</a>] 
</p></td></tr></table>

</ul>
<br />


<A NAME="Services"><h2>Services</h2></A>

<p><b>Membership</b>: </p>
<font size="3"> 
<ul>
<li><a href="https://www.ieee.org/membership/index.html" target="_blank" style="color:#2a7ce0">IEEE Membership</a>, Graduate Student Member, 2022.01-present</li>
<li><a href="http://www.ieee.org/yp" target="_blank" style="color:#2a7ce0">IEEE Young Professionals</a>, 2022.01-present</li>
<li><a href="https://www.csig.org.cn/" target="_blank" style="color:#2a7ce0">China Society of Image and Graphics (CSIG)</a>, Student Member, 2023-2026</li>
<li><a href="https://web.ccf.org.cn/" target="_blank" style="color:#2a7ce0">China Computer Federation (CCF)</a>, Student Member, 2024-2026</li>
<li><a href="https://www.aclweb.org/" target="_blank" style="color:#2a7ce0">Association for Computational Linguistics (ACL) Membership</a>, Student Member, 2025-2026</li>
<li><a href="https://www.bmva.org/" target="_blank" style="color:#2a7ce0">British Machine Vision Association (BMVA)</a>, Student Member, 2024-present</li>
</ul>
</font>
<br />

<p><b>Invited Talk</b>: </p>
<font size="3"> 
<ul>
    <li>2024/03/27: Talk on "Modern Convolutional Neural Networks" @ Chengdu Institute of Computer Application, Chinese Academy of Sciences [<a href="./Files/ppt_Modern_CNNs_20240327.pdf" target="_blank" style="color:#2a7ce0">PPT</a>]</li>
    <li>2024/03/12: Talk on "Convolution Kernel Design and Gated Attention for Modern Convolutional Neural Networks" @ ShuZiHuanYu Platform [<a href="https://cepoca.cn/lectureHall/lectureRoomDetail?liveUid=570ce4bc2687df69b7d5819550c7989a" target="_blank" style="color:#2a7ce0">Video</a>] [<a href="./Files/ppt_Convolution Kernel Design and Gated Attention for Modern Convolutional Neural Networks_20240312.pdf" target="_blank" style="color:#2a7ce0">PPT</a>] </li>
    <li>2023/12/14: Talk on "Mixup Data Augmentation for Computer Vision" @ Chongqing Technology and Business University [<a href="./Files/ppt_Mixup_Data_Augmentation_for_Computer_Vision_20231214.pdf" target="_blank" style="color:#2a7ce0">PPT</a>]</li>
    <li>2023/12/14: Talk on "Introduction to AI Research and Experience Sharing" @ Chongqing Technology and Business University</li>
</ul>
</font>
<br />

<p><b>Program Committee Member | Reviewer</b>: </p>
<font size="3"> 
<ul>
<li><b>Conference Reviewer / PC Member:</b></li>
    International Conference on Learning Representations (<b><a href="https://iclr.cc/Conferences/2024/" target="_blank">ICLR</a></b>), 2024-2025<br />
    International Conference on Machine Learning (<b><a href="https://icml.cc/" target="_blank">ICML</a></b>), 2022-2025<br />
    Conference and Workshop on Neural Information Processing Systems (<b><a href="https://neurips.cc/Conferences/2024" target="_blank">NeurIPS</a></b>), 2022-2025<br />
    Dataset and Benchmark Track on Neural Information Processing Systems (<b><a href="https://neurips.cc/Conferences/2024" target="_blank">NeurIPS DB Track</a></b>), 2023-2024<br />
    IEEE Conference on Computer Vision and Pattern Recognition (<b><a href="https://cvpr.thecvf.com/Conferences/2024" target="_blank">CVPR</a></b>), 2022-2025<br />
    International Conference on Computer Vision (<b><a href="https://iccv2023.thecvf.com/" target="_blank">ICCV</a></b>), 2023-2025<br />
    Conference on European Conference on Computer Vision (<b><a href="https://eccv2024.ecva.net/" target="_blank">ECCV</a></b>), 2022-2024<br />
    AAAI Conference on Artificial Intelligence (<b><a href="https://aaai.org/conference/aaai/aaai-25/" target="_blank">AAAI</a></b>), 2022-2026<br />
    International Joint Conference on Artificial Intelligence (<b><a href="https://ijcai24.org/" target="_blank">IJCAI</a></b>), 2023-2025<br />
    ACM Multimedia (<b><a href="https://2024.acmmm.org/" target="_blank">ACMMM</a></b>), 2022-2025<br />
    IEEE Conference on Multimedia Expo (<b><a href="https://2024.ieeeicme.org" target="_blank">ICME</a></b>), 2024<br />
    International Conference on Artificial Intelligence and Statistics (<b><a href="https://virtual.aistats.org/Conferences/2025" target="_blank">AISTATS</a></b>), 2025<br />
    British Machine Vision Conference (<b><a href="https://bmvc2024.org/" target="_blank">BMVC</a></b>), 2024-2025<br />
    <!-- International Conference on Pattern Recognition (<b><a href="https://icpr2024.org" target="_blank">ICPR</a></b>), 2024<br /> -->
    Asian Conference on Computer Vision (<b><a href="https://accv2024.org/" target="_blank">ACCV</a></b>), 2024<br />
    IEEE Winter Conference on Applications of Computer Vision (<b><a href="https://wacv2024.thecvf.com" target="_blank">WACV</a></b>), 2022-2025<br />
<li><b>Journal Reviewer:</b></li>
    IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>)<br />
    International Journal of Computer Vision (<b>IJCV</b>)<br />
    IEEE Transactions on Neural Networks and Learning Systems (<b>TNNLS</b>)<br />
    IEEE Transactions on Circuits and Systems for Video Technology (<b>TCSVT</b>)<br />
    Neural Computing & Applications (<b>NCA</b>)<br />
    Electronic Research Archive (<b>ERA</b>)<br />
</ul>
</font>
<br />

<p><b>Teaching Assistant</b>: </p>
<font size="3"> 
<ul>
<li><a href="https://github.com/Westlake-DL/DL-Course-2024" target="_blank" style="color:#2a7ce0">Deep Learning Course</a>, Westlake University, 2024.03-2024.06</li>
</ul>
</font>
<br />

<p><b>Online Project</b>: </p>
<font size="3"> 
<ul>
    <li><a href="https://arxiv.org/abs/2209.04851" target="_blank" style="color:#2a7ce0">OpenMixup</a>: Open mixup toolbox and benchmark for visual representation learning [<a href="https://github.com/Westlake-AI/openmixup" target="_blank" style="color:#2a7ce0">Code</a>]</li>
    <li><a href="https://arxiv.org/abs/2306.11249" target="_blank" style="color:#2a7ce0">OpenSTL</a>: Open-source project for video prediction benchmarks [<a href="https://github.com/chengtan9907/OpenSTL" target="_blank" style="color:#2a7ce0">Code</a>]</li>
    <li><a href="https://arxiv.org/abs/2401.00897" target="_blank" style="color:#2a7ce0">Awesome-MIM</a>: Masked Modeling for Self-supervised Representation Learning on Vision and Beyond [<a href="https://github.com/Lupin1998/Awesome-MIM" target="_blank" style="color:#2a7ce0">Code</a>]</li>
    <li><a href="https://arxiv.org/abs/2409.05202" target="_blank" style="color:#2a7ce0">Awesome-Mixup</a>: Awesome List of Mixup Augmentation and Beyond [<a href="https://github.com/Westlake-AI/Awesome-Mixup" target="_blank" style="color:#2a7ce0">Code</a>]</li>
</ul>
</font>
<br />

<p><b>Social Awards</b>: </p>
<font size="3"> 
<ul>
    <li>2025: CVPR2025 Broadening Participation Participant Award</li>
    <li>2024: Westlake University Suwu Scholarship (2024)</li>
    <li>2024: ICML2024 Scholarship (Financial Aid Awardees)</li>
    <li>2024: ICLR2024 Scholarship (Financial Assistance Awardees)</li>
    <li>2023: ICML2023 Participation Grant</li>
</ul>
</font>
<br />
<br />

</div>


<script type="text/javascript" src="//rf.revolvermaps.com/0/0/6.js?i=5x3ebj080sx&amp;m=7&amp;c=e63100&amp;cr1=ffffff&amp;f=arial&amp;l=0&amp;bv=90&amp;lx=-420&amp;ly=420&amp;hi=20&amp;he=7&amp;hc=a8ddff&amp;rs=80" async="async"></script>
 

<div id="article"></div>
<div id="back_top">
<div class="arrow"></div>
<div class="stick"></div>
</div>

<script>
$(function(){
    $(window).scroll(function(){  //If scroll
        var scrollt = document.documentElement.scrollTop + document.body.scrollTop; //Getting Height after scroll
        if( scrollt >400 )
        {  
            $("#back_top").fadeIn(400); 
        }
        else
        {
            $("#back_top").stop().fadeOut(400);
        }
    });

    $("#back_top").click(function(){ 

        $("html,body").animate({scrollTop:"0px"}, 200);

    }); 

});
</script>


All Rights Reserved by Siyuan Li. Part of page is generated by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.


<!-- <font size="2"; color="#A0A0A0";>
<p style="text-align:center">Updating time: 2022.06.01</p>
</font> -->


</body>
</html>
